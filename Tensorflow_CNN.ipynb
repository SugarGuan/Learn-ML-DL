{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SugarGuan/Learn-ML-DL/blob/master/Tensorflow_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wll_FxqPy0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "a017b5d0-c30f-4dee-eff5-357d1ded9039"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 00:45:22.660529 139951775647616 deprecation.py:323] From <ipython-input-2-5839acd4638a>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0630 00:45:22.662754 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0630 00:45:22.667889 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0630 00:45:23.011161 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0630 00:45:23.435514 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0630 00:45:23.443677 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0630 00:45:23.753119 139951775647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQPnP5_gP6yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a tensorflow sesion\n",
        "# Have to close the session when you want to re-run any part of this ipynb file!\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUxp_lkKQQC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return initial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt1dludBQqZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 卷积层\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "# 池化层（使用max 最大特征提取方式进行池化）\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6-njJttRQ9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义输入\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "x_image = tf.reshape(x, [-1,28,28,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swzk5_DnRj2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义卷积层1\n",
        "W_conv1 = weight_variable([5,5,1,32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+ b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNHgxg-iR8NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义卷积层2\n",
        "W_conv2 = weight_variable([5,5,32,64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCchUTASSVUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 全连接层\n",
        "W_fc1 = weight_variable([7 * 7 * 64 , 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSfoJoPiTA97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cf767adf-6af4-47d5-fcf9-e68724138768"
      },
      "source": [
        "# Dropout层 通过placeholder传入一个keep_prob比率进行控制\n",
        "# 训练中随机丢弃一些数据 来减轻过拟合（如果保留全部数据可以获得最好性能）\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 00:46:17.417896 139951775647616 deprecation.py:506] From <ipython-input-11-35c3bf3a9559>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AXQDYwQTr1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmax 层，用于得到最后的概率输出\n",
        "W_fc2 = weight_variable([1024 , 10])\n",
        "b_fc2 = weight_variable([10])\n",
        "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJGzvlgUCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义损失函数 使用交叉熵（cross entropy）\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv),\n",
        "                              reduction_indices=[1]))\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCEn8pnNUfCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 评测准确率"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXGAZ54JUg0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_ ,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1N8i2v6U3_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24f0a881-8ef0-4aa9-ed02-afef135edbb9"
      },
      "source": [
        "# 初始化全局变量（variables）\n",
        "tf.global_variables_initializer().run()\n",
        "for i in range(20000):\n",
        "  batch = mnist.train.next_batch(50)\n",
        "  if i % 100 == 0:\n",
        "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_:batch[1], \n",
        "                                             keep_prob: 1.0})\n",
        "    print(\"Step %d , training accuracy %g\"%(i, train_accuracy))\n",
        "  train_step.run(feed_dict={x: batch[0], y_:batch[1], keep_prob: 0.5})"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 , training accuracy 0.18\n",
            "Step 100 , training accuracy 0.88\n",
            "Step 200 , training accuracy 0.86\n",
            "Step 300 , training accuracy 0.94\n",
            "Step 400 , training accuracy 0.96\n",
            "Step 500 , training accuracy 0.96\n",
            "Step 600 , training accuracy 0.94\n",
            "Step 700 , training accuracy 0.96\n",
            "Step 800 , training accuracy 0.92\n",
            "Step 900 , training accuracy 1\n",
            "Step 1000 , training accuracy 0.98\n",
            "Step 1100 , training accuracy 0.98\n",
            "Step 1200 , training accuracy 0.96\n",
            "Step 1300 , training accuracy 0.98\n",
            "Step 1400 , training accuracy 0.98\n",
            "Step 1500 , training accuracy 0.94\n",
            "Step 1600 , training accuracy 1\n",
            "Step 1700 , training accuracy 1\n",
            "Step 1800 , training accuracy 0.92\n",
            "Step 1900 , training accuracy 0.96\n",
            "Step 2000 , training accuracy 1\n",
            "Step 2100 , training accuracy 1\n",
            "Step 2200 , training accuracy 0.98\n",
            "Step 2300 , training accuracy 0.96\n",
            "Step 2400 , training accuracy 1\n",
            "Step 2500 , training accuracy 0.98\n",
            "Step 2600 , training accuracy 0.98\n",
            "Step 2700 , training accuracy 0.98\n",
            "Step 2800 , training accuracy 1\n",
            "Step 2900 , training accuracy 1\n",
            "Step 3000 , training accuracy 0.98\n",
            "Step 3100 , training accuracy 0.98\n",
            "Step 3200 , training accuracy 0.96\n",
            "Step 3300 , training accuracy 1\n",
            "Step 3400 , training accuracy 1\n",
            "Step 3500 , training accuracy 1\n",
            "Step 3600 , training accuracy 1\n",
            "Step 3700 , training accuracy 1\n",
            "Step 3800 , training accuracy 0.98\n",
            "Step 3900 , training accuracy 1\n",
            "Step 4000 , training accuracy 1\n",
            "Step 4100 , training accuracy 0.96\n",
            "Step 4200 , training accuracy 1\n",
            "Step 4300 , training accuracy 1\n",
            "Step 4400 , training accuracy 0.98\n",
            "Step 4500 , training accuracy 1\n",
            "Step 4600 , training accuracy 1\n",
            "Step 4700 , training accuracy 1\n",
            "Step 4800 , training accuracy 1\n",
            "Step 4900 , training accuracy 0.98\n",
            "Step 5000 , training accuracy 1\n",
            "Step 5100 , training accuracy 1\n",
            "Step 5200 , training accuracy 0.98\n",
            "Step 5300 , training accuracy 1\n",
            "Step 5400 , training accuracy 1\n",
            "Step 5500 , training accuracy 1\n",
            "Step 5600 , training accuracy 1\n",
            "Step 5700 , training accuracy 0.98\n",
            "Step 5800 , training accuracy 1\n",
            "Step 5900 , training accuracy 1\n",
            "Step 6000 , training accuracy 1\n",
            "Step 6100 , training accuracy 0.98\n",
            "Step 6200 , training accuracy 0.98\n",
            "Step 6300 , training accuracy 1\n",
            "Step 6400 , training accuracy 1\n",
            "Step 6500 , training accuracy 1\n",
            "Step 6600 , training accuracy 1\n",
            "Step 6700 , training accuracy 1\n",
            "Step 6800 , training accuracy 0.96\n",
            "Step 6900 , training accuracy 1\n",
            "Step 7000 , training accuracy 1\n",
            "Step 7100 , training accuracy 1\n",
            "Step 7200 , training accuracy 0.98\n",
            "Step 7300 , training accuracy 1\n",
            "Step 7400 , training accuracy 1\n",
            "Step 7500 , training accuracy 1\n",
            "Step 7600 , training accuracy 1\n",
            "Step 7700 , training accuracy 1\n",
            "Step 7800 , training accuracy 1\n",
            "Step 7900 , training accuracy 1\n",
            "Step 8000 , training accuracy 1\n",
            "Step 8100 , training accuracy 1\n",
            "Step 8200 , training accuracy 0.98\n",
            "Step 8300 , training accuracy 0.96\n",
            "Step 8400 , training accuracy 0.98\n",
            "Step 8500 , training accuracy 0.96\n",
            "Step 8600 , training accuracy 1\n",
            "Step 8700 , training accuracy 1\n",
            "Step 8800 , training accuracy 1\n",
            "Step 8900 , training accuracy 0.98\n",
            "Step 9000 , training accuracy 1\n",
            "Step 9100 , training accuracy 1\n",
            "Step 9200 , training accuracy 1\n",
            "Step 9300 , training accuracy 1\n",
            "Step 9400 , training accuracy 1\n",
            "Step 9500 , training accuracy 1\n",
            "Step 9600 , training accuracy 1\n",
            "Step 9700 , training accuracy 1\n",
            "Step 9800 , training accuracy 1\n",
            "Step 9900 , training accuracy 1\n",
            "Step 10000 , training accuracy 1\n",
            "Step 10100 , training accuracy 1\n",
            "Step 10200 , training accuracy 0.98\n",
            "Step 10300 , training accuracy 1\n",
            "Step 10400 , training accuracy 0.98\n",
            "Step 10500 , training accuracy 1\n",
            "Step 10600 , training accuracy 0.98\n",
            "Step 10700 , training accuracy 0.98\n",
            "Step 10800 , training accuracy 1\n",
            "Step 10900 , training accuracy 1\n",
            "Step 11000 , training accuracy 1\n",
            "Step 11100 , training accuracy 1\n",
            "Step 11200 , training accuracy 1\n",
            "Step 11300 , training accuracy 1\n",
            "Step 11400 , training accuracy 1\n",
            "Step 11500 , training accuracy 1\n",
            "Step 11600 , training accuracy 1\n",
            "Step 11700 , training accuracy 1\n",
            "Step 11800 , training accuracy 0.98\n",
            "Step 11900 , training accuracy 1\n",
            "Step 12000 , training accuracy 0.98\n",
            "Step 12100 , training accuracy 1\n",
            "Step 12200 , training accuracy 1\n",
            "Step 12300 , training accuracy 1\n",
            "Step 12400 , training accuracy 1\n",
            "Step 12500 , training accuracy 1\n",
            "Step 12600 , training accuracy 1\n",
            "Step 12700 , training accuracy 1\n",
            "Step 12800 , training accuracy 1\n",
            "Step 12900 , training accuracy 0.98\n",
            "Step 13000 , training accuracy 1\n",
            "Step 13100 , training accuracy 1\n",
            "Step 13200 , training accuracy 1\n",
            "Step 13300 , training accuracy 1\n",
            "Step 13400 , training accuracy 1\n",
            "Step 13500 , training accuracy 0.96\n",
            "Step 13600 , training accuracy 1\n",
            "Step 13700 , training accuracy 1\n",
            "Step 13800 , training accuracy 1\n",
            "Step 13900 , training accuracy 1\n",
            "Step 14000 , training accuracy 1\n",
            "Step 14100 , training accuracy 0.98\n",
            "Step 14200 , training accuracy 1\n",
            "Step 14300 , training accuracy 1\n",
            "Step 14400 , training accuracy 1\n",
            "Step 14500 , training accuracy 1\n",
            "Step 14600 , training accuracy 1\n",
            "Step 14700 , training accuracy 1\n",
            "Step 14800 , training accuracy 1\n",
            "Step 14900 , training accuracy 0.98\n",
            "Step 15000 , training accuracy 1\n",
            "Step 15100 , training accuracy 1\n",
            "Step 15200 , training accuracy 1\n",
            "Step 15300 , training accuracy 1\n",
            "Step 15400 , training accuracy 1\n",
            "Step 15500 , training accuracy 1\n",
            "Step 15600 , training accuracy 1\n",
            "Step 15700 , training accuracy 1\n",
            "Step 15800 , training accuracy 1\n",
            "Step 15900 , training accuracy 1\n",
            "Step 16000 , training accuracy 1\n",
            "Step 16100 , training accuracy 1\n",
            "Step 16200 , training accuracy 1\n",
            "Step 16300 , training accuracy 1\n",
            "Step 16400 , training accuracy 0.98\n",
            "Step 16500 , training accuracy 1\n",
            "Step 16600 , training accuracy 1\n",
            "Step 16700 , training accuracy 1\n",
            "Step 16800 , training accuracy 1\n",
            "Step 16900 , training accuracy 1\n",
            "Step 17000 , training accuracy 1\n",
            "Step 17100 , training accuracy 1\n",
            "Step 17200 , training accuracy 1\n",
            "Step 17300 , training accuracy 1\n",
            "Step 17400 , training accuracy 1\n",
            "Step 17500 , training accuracy 1\n",
            "Step 17600 , training accuracy 1\n",
            "Step 17700 , training accuracy 1\n",
            "Step 17800 , training accuracy 1\n",
            "Step 17900 , training accuracy 1\n",
            "Step 18000 , training accuracy 1\n",
            "Step 18100 , training accuracy 1\n",
            "Step 18200 , training accuracy 1\n",
            "Step 18300 , training accuracy 1\n",
            "Step 18400 , training accuracy 1\n",
            "Step 18500 , training accuracy 1\n",
            "Step 18600 , training accuracy 1\n",
            "Step 18700 , training accuracy 1\n",
            "Step 18800 , training accuracy 1\n",
            "Step 18900 , training accuracy 1\n",
            "Step 19000 , training accuracy 1\n",
            "Step 19100 , training accuracy 1\n",
            "Step 19200 , training accuracy 1\n",
            "Step 19300 , training accuracy 1\n",
            "Step 19400 , training accuracy 1\n",
            "Step 19500 , training accuracy 1\n",
            "Step 19600 , training accuracy 1\n",
            "Step 19700 , training accuracy 1\n",
            "Step 19800 , training accuracy 1\n",
            "Step 19900 , training accuracy 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlU3toY4Wc6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6375ea96-9d1c-42e8-ec99-aec842a23da6"
      },
      "source": [
        "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
        "    x: mnist.test.images, y_:mnist.test.labels, keep_prob: 1.0}))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKHSLLEVgefx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # tf.InteractiveSession.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}